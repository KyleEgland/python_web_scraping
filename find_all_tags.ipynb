{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Target: http://dataquestio.github.io/web-scraping-pages/simple.html\n"
     ]
    }
   ],
   "source": [
    "# This code is based off a tutorial found at:\n",
    "# https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "# This bit of code builds upon parse_with_bs4.py in order to demonstrate\n",
    "# parsing of the HTML in a more simplified fashio.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Designating a 'target', not neccessary to do as variable\n",
    "target = \"http://dataquestio.github.io/web-scraping-pages/simple.html\"\n",
    "print('[*] Target: {}'.format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Requesting page.\n",
      "[*] Status code: 200\n"
     ]
    }
   ],
   "source": [
    "# Request a webpage using the requests.get() method\n",
    "print('[*] Requesting page.')\n",
    "page = requests.get(target)\n",
    "\n",
    "# The response object also has a status code, indicating that the page\n",
    "# downloaded successfully (status code 200)\n",
    "print('[*] Status code: {}'.format(page.status_code))\n",
    "\n",
    "# Pass the page downloaded to BeautifulSoup - specifically, the HTML\n",
    "soup = bs(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Find all <p> tags:\n",
      "[<p>Here is some simple content for this page.</p>]\n",
      "[*] Grab just text from <p> tag:\n",
      "Here is some simple content for this page.\n",
      "[*] Get first instance of <p> tag:\n",
      "<p>Here is some simple content for this page.</p>\n"
     ]
    }
   ],
   "source": [
    "# Find all of the paragraph, 'p', tags.\n",
    "print('[*] Find all <p> tags:')\n",
    "print(soup.find_all('p'))\n",
    "# The 'find_all' returns a list.  Loop through list or use list indexing to\n",
    "# extract the text.\n",
    "print('[*] Grab just text from <p> tag:')\n",
    "print(soup.find_all('p')[0].get_text())\n",
    "\n",
    "# The first instance of a tag can be found by using the 'find' method.  This\n",
    "# returns a BeautifulSoup object:\n",
    "print('[*] Get first instance of <p> tag:')\n",
    "print(soup.find('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
